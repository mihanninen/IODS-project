---
---
---

# Assignment 4 - Clustering and classification

```{r}

date()

# install.packages(c("MASS", "corrplot", "tidyverse", "reshape2")) if needed

```

## Exploring our data

In this assignment we look at the `Boston` data set from the `MASS` library. It describes housing values in suburbs of Boston and various other factors relating to the housings and locations. The data frame has 14 variables and 506 observations (14 columns and 506 rows). Here I show the structure of the data frame and a short summary of the variables in question.

```{r}
# access the MASS package
library(MASS)

# load the Boston data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)
```

## Graphical overview of data

Here I visualize the distribution of the variables and the relationships between them.

```{r}
# access the required libraries
library(tidyr)
library(reshape2)
library(ggplot2)
library(corrplot)

# convert data from wide to long
melt.boston <- melt(Boston)

# visualize variables with small multiple chart
g1 <- ggplot(data = melt.boston, aes(x = value)) + 
stat_density() + 
facet_wrap(~variable, scales = "free")
g1

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(2)

# test correlation 
testRes = cor.mtest(cor_matrix, conf.level = 0.95)

# visualize the correlation matrix
corrplot.mixed(cor_matrix, tl.col = "black", lower.col = "black", number.cex = 0.7, tl.cex=0.7)

```

Looking at the previous summary of the variables and visualization afterwards, we can get a good understanding of the data. We can see that crime rate `crim`, proportion of residential lands zoned `zn`, distance to employment centers `dis` and lower status of the population `lstat` are all clearly skewed right. On the other side, proportion of blacks `black`, proportion of units built before 1940 `age` and pupil-teacher ratio `ptratio` are all clearly skewed left. Porportion of non-retail business acres `indus`, accessability to highways `rad` and property-tax rate `tax` have bimodal distributions (Charles River dummy variable `char` is a binary variable). Looking at the correlation matrix, we can see that the highest positive correlations are between `tax` and `rad`, `indus` and `nox` (nitrogen oxide concentration), `age` and `nox`, and `rm` (average number of rooms) and `medv` (median value). We can also see that the highest negative correlations are between `dis` and `nox`, `dis` and `age`, `medv`and `lstat`, and `dis` and `indus`.

## Standardizing the data

In order to standardize our data we have to scale the data.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# print summary of scaled variables
summary(boston_scaled)

```

After scaling the data we can notice that the means of all variables are 0, meaning that the scale has been fitted for each variable so that the range of each variable is approximately the same.

Next we create a categorical variable of the crime rate in the scaled data set. After this, we divide the data set to train and test sets (80% of the data belongs to the train set).

```{r}
#access the dplyr library
library(dplyr)

# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

## Fitting the linear discriminant analysis

Here we fit a linear discriminant analysis (LDA) on the train set, using the categorical crime rate `crime` as the target variable and all the other variables in the data set as predictor variables. We then visualize this analysis with a biplot.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```
## Predicting classes using the LDA model

Next we first save the crime categories from the test set and then remove the categorical crime variable from the test data set. AFter this, we predict the classes with the LDA model on the test data. Finally we cross tabulate the results with the crime categories from the test set.

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Looking at the cross tabulation we can see that the overall prediction rate of our model is approximately 68% (correct predictions devided by all predictions), which is fairly good. We notice that our model is really good at predicting high crime rate, while not as good at predicting the other categories.

## Measuring distance and clustering

Next we reload the `Boston` data set and standardize it. Then we calculate the distance between observations.

```{r}
library(MASS)
data("Boston")

# center and standardize variables as done previously
boston_scaled2 <- scale(Boston)

# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)

# create the euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)
```

Next we run the k-means algorithm on the data set and after finding the optimal number of clusters we visualize them.

```{r}
# Use seed to mitigate the effect of random number generators
set.seed(1234)

# k-means clustering
km <- kmeans(Boston, centers = 3)

# plot the scaled data set with clusters
pairs(boston_scaled2, col = km$cluster)

# determine the number of clusters for optimization
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering with optimal number of clusters
km2 <- kmeans(boston_scaled2, centers = 2)

# plot the scaled dataset with clusters
pairs(boston_scaled2, col = km2$cluster)

```

The k-means alorithm was initially done with three clusters. Visualizing how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes, we can see the total WCSS drops radically at two clusters meaning this is our optimal number of clusters. We then preformed the k-means algorithm again with the optimal number of clusters and visualized it. Looking at our plot with two clusters we can see that overall the clusters are fairly well separated within the variables, meaning the clustering seems to work properly. Comparing the final plot to the original k-mean algorith with 3 clusters we can see that there is more overlap between clusters and is therefore harder to interpret.

