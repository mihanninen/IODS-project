---
---
---

# Assignment 2

*Describe the work you have done this week and summarize your learning.*

-   Describe your work and results clearly.
-   Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
-   Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

```{r}

date()

```

## 1. Part

The data I'm using in my analysis is derived from the larger "JYTOPKYS2"-dataset. This dataset was collected from a survey to statistics students, that was used to evaluate to effects of learning approaches to exam results.

Seven variables from the original dataset were chosen for this data (gender, age, attitude, deep learning, strategic learning, surface learning and exam points). Additionally, students who scored 0 in their exam were excluded from this data.

The variable `attitude` in `learning2014` is a sum of 10 questions related to students attitude towards statistics, each measured on the [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) (1-5).

Variables `deep` (deep learning), `stra`(strategic learning) and `surf`(surface learning) in `learning2014` are the mean values from the combinations of questions that relate to each learning approach, respectively. Each question was once again measured on the Likert scale (1-5).

In the following R chunk I will explore the structure and dimensions of the data further.

```{r part 1}
# reading the data into memory
learning2014 <- read.csv("~/Documents/Tohtoritutkinto/Open Science/IODS-project/data/learning2014.csv", sep = ",", header = TRUE)

#displaying dataframe
learning2014

# the dataset has 166 rows and 7 columns
dim(learning2014)

# there are 7 variables and 166 observations. One variable is a character string, while the other variables are integers or numbers
str(learning2014)

# a summary of the variables
summary(learning2014)
```

## 2. Part

Here I present a graphical overview of the data and show summaries of the variables in the data.

```{r part 2}
# accessing the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# creating a plot matrix to give a graphical overview of the data
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

# drawing the plot
p

# creating summaries of the variables
summary(factor(learning2014$gender))
summary(learning2014$age)
summary(learning2014$attitude)
summary(learning2014$deep)
summary(learning2014$stra)
summary(learning2014$surf)
summary(learning2014$points)
```

The data is divided in the plot matrix by gender, with females depicted by the color red and males by the color green. Looking at the distributions of variables in the graphical overview, you can quickly notice that the `age`-variable is strongly skewed to the right. This is expected as the study population consists of students. We also notice a slight left skew in deep learning (`deep`) and the `points`-variable. The `attitude`-variable is slightly left skewed in males, but almost normal distribution in females. Additionally, suface learning (`surf`) is right skewed in males, but again closer to normal distribution in females.

We can further appreciate the distributions and characteristics of individual variables by looking at the scatter and box plots of the plot matrix. For example, you could inspect if a certain variable has lots of outliers in the box plot or if spread is larger or different in the scatter plot.

The graphical overview also shows correlation coefficients between variables. The most notable of these is the strong positive correlation between `attitude` and `points`. Also worth noting is the strong negative correlation between surface learning (`surf`) and both `attitude`and deep learning (`deep`) in male students that is absent in female students. There is also a significant negative correlation between surface learning (`surf`) and strategic learning (`stra`) when the students are analyzied overall.

Looking at the summaries of the variables, you notice that the median age is quite young (median = 22 years) and that there are almost twice as many females as there are men (110 females to 56 males).

## 3. Part

Here I create a regression model where exam points is the target variable.

```{r part 3}
# accessing the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# creating a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + surf, data = learning2014)

summary(my_model)

# creating new model without non-significant explanatory variable surf
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
summary(my_model2)

```
My initial model had attitude, strategic and surface learning as explanatory variables, as they seemed to have the largest correlation coefficients in the plot matrix. Looking at the coefficients, we notice that there are positive estimates in attitude and strategic learning, while surface learning has a negative estimate. The t-statistic is the coefficient divided by the standard error. The larger the t-statistic value is, the more likely it is that the coefficient isn't 0 and that there is a relationship between the variables. The t-value of `attitude` is clearly above 0, meaning there is a possibility that we can reject the null hypothesis (no relationship between target variable and explanatory variable) and declare a relationship between `points` and `attitude`. This is supported by the small Pr(\>\|t\|) in attribute, meaning there is a very small chance of seeing a relationship between `points` and `attitude` due to chance. The same cannot be said about strategic and surface learning. While strategic learning does have a t-value above 0, the probability that this is due to chace is above the cut-off point (p \<0,05) as `stra` has a Pr(\>\|t\|) value of 0.11716. Surface learning is even more likely to have a relationship due to chance as `surf` has a t-value of -0.731 and a Pr(\>\|t\|) value of 0.11716.

After the first model, I removed the `surf`variable from the model since it didn't have a significant relationship with the target variable `points`. The new model was fitted with only Â´attitude`and`stra` as explanatory variables.

## 4. Part

Next I use the summary of my new fitted model and explain the relationship between the chosen explanatory variables and the target variable.

```{r part 4}
# accessing the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# my new model without the non-significant explanatory variable surf and its summary
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
summary(my_model2)
```
In my new model we notice that both attitude and strategic learning have still positive estimates (0.34658 and 0.91365 respectively). This means that for every point the `attitude`-variable increases, the exam points increase 0.34658 on average (and 0.91365 in the case of `stra`). The t-statistics of attitude is clearly above 0 (t-value: 6.132), meaning there is a high possibility that we can declare a relationship between `points` and `attitude`. This is supported by the small Pr(\>\|t\|) in `attribute` (Pr(\>\|t\|): 6.31e-09), meaning there is a very small chance of seeing a relationship between `points` and `attitude` due to chance. Even though the t-statistics of `stra` is also clearly above 0 (t-value: 1.709) and the Pr(\>\|t\|) is smaller than before (Pr(\>\|t\|): 0.08927), it still falls short of the default cut-off point of p \<0.05. Therefore, we cannot say that there is a significant relationship between `points`and `stra`. Rather there is strong evidence of a potential relationship, but there is approximately an 8,9% probability that this is due to chance.

The R-squared statistic provides a measure of how well the model fits data. A model with a R-squared value of 0 doesn't explain at all the variance in exam points and on the other hand a model with a R-squared value of 1 explains all of the variance in exam points. The square of the multiple correlation coefficient (multiple R-squared) of the model is 0.2048. As the multiple R-squared value increases with each additional explanatory variable, it is preferred to use the adjusted R-squared as it adjusts the value for the numbers of variables considered. The adjusted square of the multiple correlation coefficient (adjusted R-squared) is 0.1951, i.e. these variables accounted for about 20% of the variation in the exam points. This is marginally more than in the previous model. The residual standard error is a measure of the quality of the linear regression fit. It is the average amount that the target variable `points` will deviate from the true regression line. The smaller the residual standard error the better the model fits the dataset. In this model the residual standard error is 5.289, which is slightly better than the first model. The omnibus F-test had a very low associated p-value (p-value: 7.734e-09), so there is very strong evidence that neither of the two regression coefficients are zero.

## 5. Part

Finally I produce the following diagnostic plots of my model: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.

```{r}
# divides the window into a 2x2 grid
par(mfrow = c(2,2))

# draws the diagnostic plots using the plot() function. The previously specified plots 1, 2 and 5 are selected
plot(my_model2, c(1,2,5))
```

Linear regression has four main assumptions: linear relationship, independence, homoscedasticity and normality.

We can use the residuals vs. fitted plot to evaluate if the there is correlation between residuals and fitted values. As we can see, the datapoints have a fairly random distribution around the 0 line, which supports the hypothesis that it is linear and the data is reasonably homoscedastic. There are a few outliers with large negative residuals, but these don't appear to alter the overall variance in the data.

Using the normal Q-Q plot, we can evaluate the skewness of data and fit of model. My model seems to be slightly skewed left, but is fairly normally distributed overall.

Finally, we can use the residuals vs. leverage plot to assess linearity and influential points. Influential points are observations, that when removed from the dataset could change the coefficients of the model when fitting again. Here we can see that the the spread is nicely linear. No influentials points are observed in the spread.
